#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BossJy-Pro æ•°æ®å¯¼å…¥å’Œç»Ÿè®¡å·¥å…·
å°†dataç›®å½•ä¸‹çš„æ‰€æœ‰æ•°æ®å¯¼å…¥PostgreSQLæ•°æ®åº“ï¼Œå¹¶æä¾›BotæŸ¥è¯¢åŠŸèƒ½
"""

import os
import sys
import pandas as pd
import psycopg2
from psycopg2.extras import execute_values, execute_batch
import logging
from pathlib import Path
import glob
import json
from datetime import datetime
from typing import Dict, List, Tuple, Any
import hashlib

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/data_import.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# æ•°æ®åº“é…ç½®
import re
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
env_path = Path(__file__).parent.parent / '.env'
load_dotenv(env_path)

# ä»DATABASE_URLè§£æé…ç½®
DATABASE_URL = os.getenv('DATABASE_URL', 'os.environ.get('DATABASE_URL', 'postgresql://bossjy:CHANGE_ME@postgres:5432/bossjy')')
# å¤„ç†Docker hoståç§°
DATABASE_URL = DATABASE_URL.replace('@postgres:', '@localhost:')
# å¼ºåˆ¶ä½¿ç”¨æ­£ç¡®çš„ç«¯å£
DATABASE_URL = DATABASE_URL.replace(':5432/', ':15432/')

# è§£æDATABASE_URL
print(f"DATABASE_URL: {DATABASE_URL}")
match = re.match(r'postgresql://([^:]+):([^@]+)@([^:]+):(\d+)/(.+)', DATABASE_URL)
if match:
    user, password, host, port, database = match.groups()
    DB_CONFIG = {
        'host': host,
        'port': int(port),
        'database': database,
        'user': user,
        'password': password
    }
    print(f"è§£æçš„æ•°æ®åº“é…ç½®: {DB_CONFIG}")
else:
    # é»˜è®¤é…ç½®
    DB_CONFIG = {
        'host': 'localhost',
        'port': 15432,
        'database': 'bossjy_huaqiao',
        'user': 'bossjy',
        'password': 'ji394su3'
    }
    print(f"ä½¿ç”¨é»˜è®¤æ•°æ®åº“é…ç½®: {DB_CONFIG}")

class DataImporter:
    """æ•°æ®å¯¼å…¥å™¨"""
    
    def __init__(self):
        self.data_dir = Path("data")
        self.stats = {
            'total_files': 0,
            'imported_files': 0,
            'failed_files': 0,
            'total_records': 0,
            'imported_records': 0,
            'categories': {},
            'regions': {},
            'sources': {}
        }
        
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        Path("logs").mkdir(exist_ok=True)
        
    def get_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥"""
        return psycopg2.connect(**DB_CONFIG)
    
    def create_tables(self):
        """åˆ›å»ºæ•°æ®è¡¨"""
        logger.info("åˆ›å»ºæ•°æ®è¡¨...")
        conn = self.get_connection()
        cursor = conn.cursor()
        
        try:
            # åˆ›å»ºä¸»æ•°æ®è¡¨
            cursor.execute("""
                DROP TABLE IF EXISTS chinese_data CASCADE;
                CREATE TABLE chinese_data (
                    id BIGSERIAL PRIMARY KEY,
                    original_name VARCHAR(500),
                    chinese_name VARCHAR(500),
                    english_name VARCHAR(500),
                    phone_number VARCHAR(100),
                    email VARCHAR(255),
                    address TEXT,
                    city VARCHAR(200),
                    region VARCHAR(100),
                    country VARCHAR(100),
                    category VARCHAR(100),
                    source_file VARCHAR(500),
                    file_hash VARCHAR(64),
                    row_number INTEGER,
                    confidence_score DECIMAL(3,2),
                    created_at TIMESTAMP DEFAULT NOW(),
                    updated_at TIMESTAMP DEFAULT NOW(),
                    
                    -- ç´¢å¼•
                    INDEX idx_original_name (original_name),
                    INDEX idx_chinese_name (chinese_name),
                    INDEX idx_phone_number (phone_number),
                    INDEX idx_email (email),
                    INDEX idx_region (region),
                    INDEX idx_category (category),
                    INDEX idx_source_file (source_file),
                    INDEX idx_confidence_score (confidence_score),
                    
                    -- å…¨æ–‡æœç´¢ç´¢å¼•
                    INDEX idx_full_text_search USING gin(to_tsvector('chinese', original_name || ' ' || COALESCE(chinese_name, '') || ' ' || COALESCE(address, '')))
                );
                
                -- åˆ›å»ºç»Ÿè®¡è¡¨
                CREATE TABLE IF NOT EXISTS data_statistics (
                    id SERIAL PRIMARY KEY,
                    category VARCHAR(100),
                    region VARCHAR(100),
                    country VARCHAR(100),
                    source_file VARCHAR(500),
                    record_count INTEGER,
                    import_time TIMESTAMP DEFAULT NOW()
                );
                
                -- åˆ›å»ºæŸ¥è¯¢ç¼“å­˜è¡¨
                CREATE TABLE IF NOT EXISTS query_cache (
                    id SERIAL PRIMARY KEY,
                    query_key VARCHAR(255) UNIQUE,
                    query_result JSONB,
                    created_at TIMESTAMP DEFAULT NOW(),
                    expires_at TIMESTAMP
                );
                
                -- åˆ›å»ºæ•°æ®æ¥æºè¡¨
                CREATE TABLE IF NOT EXISTS data_sources (
                    id SERIAL PRIMARY KEY,
                    source_file VARCHAR(500) UNIQUE,
                    file_path TEXT,
                    file_size BIGINT,
                    file_hash VARCHAR(64),
                    import_status VARCHAR(50),
                    record_count INTEGER,
                    import_time TIMESTAMP DEFAULT NOW(),
                    last_updated TIMESTAMP DEFAULT NOW()
                );
            """)
            
            conn.commit()
            logger.info("æ•°æ®è¡¨åˆ›å»ºå®Œæˆ")
            
        except Exception as e:
            conn.rollback()
            logger.error(f"åˆ›å»ºæ•°æ®è¡¨å¤±è´¥: {e}")
            raise
        finally:
            conn.close()
    
    def calculate_file_hash(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    
    def detect_category_and_region(self, file_path: Path) -> Tuple[str, str]:
        """æ£€æµ‹æ–‡ä»¶ç±»åˆ«å’Œåœ°åŒº"""
        file_name = file_path.name.lower()
        file_path_str = str(file_path).lower()
        
        # æ£€æµ‹ç±»åˆ«
        category = "å…¶ä»–"
        if "åäºº" in file_name or "chinese" in file_name:
            category = "åäºº"
        elif "é¦™æ¸¯" in file_name or "hongkong" in file_name or "hk" in file_name:
            category = "é¦™æ¸¯äºº"
        elif "å°å°¼" in file_name or "indonesia" in file_name or "indonesian" in file_name:
            category = "å°å°¼åäºº"
        elif "æ¾³æ´²" in file_name or "australia" in file_name or "au" in file_name:
            category = "æ¾³æ´²åäºº"
        elif "åŠ æ‹¿å¤§" in file_name or "canada" in file_name or "ca" in file_name:
            category = "åŠ æ‹¿å¤§åäºº"
        
        # æ£€æµ‹åœ°åŒº
        region = "å…¶ä»–"
        if "é¦™æ¸¯" in file_path_str or "hongkong" in file_path_str:
            region = "é¦™æ¸¯"
        elif "æ¾³æ´²" in file_path_str or "australia" in file_path_str:
            region = "æ¾³æ´²"
        elif "å°å°¼" in file_path_str or "indonesia" in file_path_str:
            region = "å°å°¼"
        elif "åŠ æ‹¿å¤§" in file_path_str or "canada" in file_path_str:
            region = "åŠ æ‹¿å¤§"
        elif "å°æ¹¾" in file_path_str or "taiwan" in file_path_str:
            region = "å°æ¹¾"
        elif "æ–°åŠ å¡" in file_path_str or "singapore" in file_path_str:
            region = "æ–°åŠ å¡"
        
        return category, region
    
    def extract_name_fields(self, df: pd.DataFrame, file_path: Path) -> Dict[str, str]:
        """æå–å§“åå­—æ®µ"""
        name_fields = {}
        columns = df.columns.tolist()
        
        # æŸ¥æ‰¾å¯èƒ½çš„å§“ååˆ—
        for col in columns:
            col_lower = str(col).lower()
            
            # åŸå§‹å§“å
            if any(keyword in col_lower for keyword in ['name', 'å§“å', 'nama', 'full_name', 'full name']):
                if df[col].notna().any():
                    name_fields['original_name'] = col
            
            # ä¸­æ–‡å
            if any(keyword in col_lower for keyword in ['chinese', 'ä¸­æ–‡', 'ä¸­æ–‡å', 'chinese_name', 'chinese name']):
                if df[col].notna().any():
                    name_fields['chinese_name'] = col
            
            # è‹±æ–‡å
            if any(keyword in col_lower for keyword in ['english', 'è‹±æ–‡', 'è‹±æ–‡å', 'english_name', 'english name']):
                if df[col].notna().any():
                    name_fields['english_name'] = col
        
        return name_fields
    
    def extract_contact_fields(self, df: pd.DataFrame, file_path: Path) -> Dict[str, str]:
        """æå–è”ç³»æ–¹å¼å­—æ®µ"""
        contact_fields = {}
        columns = df.columns.tolist()
        
        for col in columns:
            col_lower = str(col).lower()
            
            # ç”µè¯
            if any(keyword in col_lower for keyword in ['phone', 'ç”µè¯', 'mobile', 'æ‰‹æœº', 'contact', 'è”ç³»æ–¹å¼']):
                if df[col].notna().any():
                    contact_fields['phone_number'] = col
            
            # é‚®ç®±
            if any(keyword in col_lower for keyword in ['email', 'é‚®ç®±', 'mail', 'ç”µå­é‚®ä»¶']):
                if df[col].notna().any():
                    contact_fields['email'] = col
            
            # åœ°å€
            if any(keyword in col_lower for keyword in ['address', 'åœ°å€', 'street', 'è¡—é“']):
                if df[col].notna().any():
                    contact_fields['address'] = col
            
            # åŸå¸‚
            if any(keyword in col_lower for keyword in ['city', 'åŸå¸‚', 'town', 'åŸé•‡']):
                if df[col].notna().any():
                    contact_fields['city'] = col
        
        return contact_fields
    
    def calculate_confidence_score(self, row: pd.Series, category: str, name_fields: Dict[str, str]) -> float:
        """è®¡ç®—ç½®ä¿¡åº¦åˆ†æ•°"""
        score = 0.0
        
        # åŸºç¡€åˆ†æ•°
        if category == "åäºº":
            score += 0.3
        elif category == "é¦™æ¸¯äºº":
            score += 0.4
        elif category == "å°å°¼åäºº":
            score += 0.35
        elif category == "æ¾³æ´²åäºº":
            score += 0.3
        
        # å§“åå­—æ®µå®Œæ•´æ€§
        if 'original_name' in name_fields and pd.notna(row.get(name_fields['original_name'])):
            score += 0.2
        if 'chinese_name' in name_fields and pd.notna(row.get(name_fields['chinese_name'])):
            score += 0.3
        
        # è”ç³»æ–¹å¼å®Œæ•´æ€§
        if pd.notna(row.get('phone_number')):
            score += 0.1
        if pd.notna(row.get('email')):
            score += 0.1
        
        return min(score, 1.0)
    
    def import_file(self, file_path: Path) -> int:
        """å¯¼å…¥å•ä¸ªæ–‡ä»¶"""
        logger.info(f"å¯¼å…¥æ–‡ä»¶: {file_path}")
        
        try:
            # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
            file_hash = self.calculate_file_hash(file_path)
            file_size = file_path.stat().st_size
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å¯¼å…¥
            conn = self.get_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT id FROM data_sources WHERE file_hash = %s", (file_hash,))
            if cursor.fetchone():
                logger.info(f"æ–‡ä»¶å·²å¯¼å…¥ï¼Œè·³è¿‡: {file_path}")
                conn.close()
                return 0
            
            # æ£€æµ‹ç±»åˆ«å’Œåœ°åŒº
            category, region = self.detect_category_and_region(file_path)
            
            # è¯»å–æ–‡ä»¶
            if file_path.suffix.lower() in ['.xlsx', '.xls']:
                df = pd.read_excel(file_path)
            elif file_path.suffix.lower() == '.csv':
                try:
                    df = pd.read_csv(file_path, encoding='utf-8')
                except UnicodeDecodeError:
                    try:
                        df = pd.read_csv(file_path, encoding='gbk')
                    except UnicodeDecodeError:
                        df = pd.read_csv(file_path, encoding='latin1')
            else:
                logger.warning(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path}")
                conn.close()
                return 0
            
            # æå–å­—æ®µæ˜ å°„
            name_fields = self.extract_name_fields(df, file_path)
            contact_fields = self.extract_contact_fields(df, file_path)
            
            # å‡†å¤‡æ•°æ®
            records = []
            for idx, row in df.iterrows():
                try:
                    # æå–å§“å
                    original_name = ""
                    if 'original_name' in name_fields:
                        original_name = str(row.get(name_fields['original_name'], "")).strip()
                    
                    chinese_name = ""
                    if 'chinese_name' in name_fields:
                        chinese_name = str(row.get(name_fields['chinese_name'], "")).strip()
                    
                    english_name = ""
                    if 'english_name' in name_fields:
                        english_name = str(row.get(name_fields['english_name'], "")).strip()
                    
                    # æå–è”ç³»æ–¹å¼
                    phone_number = ""
                    if 'phone_number' in contact_fields:
                        phone_number = str(row.get(contact_fields['phone_number'], "")).strip()
                    
                    email = ""
                    if 'email' in contact_fields:
                        email = str(row.get(contact_fields['email'], "")).strip()
                    
                    address = ""
                    if 'address' in contact_fields:
                        address = str(row.get(contact_fields['address'], "")).strip()
                    
                    city = ""
                    if 'city' in contact_fields:
                        city = str(row.get(contact_fields['city'], "")).strip()
                    
                    # è®¡ç®—ç½®ä¿¡åº¦
                    confidence_score = self.calculate_confidence_score(row, category, name_fields)
                    
                    # åˆ›å»ºè®°å½•
                    record = (
                        original_name or None,
                        chinese_name or None,
                        english_name or None,
                        phone_number or None,
                        email or None,
                        address or None,
                        city or None,
                        region,
                        None,  # country
                        category,
                        file_path.name,
                        file_hash,
                        idx + 1,
                        confidence_score
                    )
                    
                    records.append(record)
                    
                except Exception as e:
                    logger.warning(f"å¤„ç†è¡Œæ•°æ®å¤±è´¥: {e}")
                    continue
            
            # æ‰¹é‡æ’å…¥
            if records:
                execute_values(
                    cursor,
                    """
                    INSERT INTO chinese_data
                    (original_name, chinese_name, english_name, phone_number, email,
                     address, city, region, country, category, source_file,
                     file_hash, row_number, confidence_score)
                    VALUES %s
                    """,
                    records,
                    page_size=1000
                )
                
                # è®°å½•æ–‡ä»¶ä¿¡æ¯
                cursor.execute("""
                    INSERT INTO data_sources
                    (source_file, file_path, file_size, file_hash, import_status, record_count)
                    VALUES (%s, %s, %s, %s, %s, %s)
                    ON CONFLICT (source_file) DO UPDATE SET
                    record_count = EXCLUDED.record_count,
                    last_updated = NOW()
                """, (file_path.name, str(file_path), file_size, file_hash, 'completed', len(records)))
                
                # æ›´æ–°ç»Ÿè®¡
                cursor.execute("""
                    INSERT INTO data_statistics
                    (category, region, record_count)
                    VALUES (%s, %s, %s)
                """, (category, region, len(records)))
            
            conn.commit()
            conn.close()
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.stats['imported_records'] += len(records)
            self.stats['categories'][category] = self.stats['categories'].get(category, 0) + len(records)
            self.stats['regions'][region] = self.stats['regions'].get(region, 0) + len(records)
            self.stats['sources'][file_path.name] = len(records)
            
            logger.info(f"æ–‡ä»¶å¯¼å…¥å®Œæˆ: {file_path}, è®°å½•æ•°: {len(records)}")
            return len(records)
            
        except Exception as e:
            logger.error(f"å¯¼å…¥æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            if 'conn' in locals():
                conn.rollback()
                conn.close()
            return 0
    
    def import_all_data(self):
        """å¯¼å…¥æ‰€æœ‰æ•°æ®"""
        logger.info("å¼€å§‹å¯¼å…¥æ‰€æœ‰æ•°æ®...")
        
        # åˆ›å»ºè¡¨
        self.create_tables()
        
        # æŸ¥æ‰¾æ‰€æœ‰æ–‡ä»¶
        all_files = []
        
        # æŸ¥æ‰¾CSVæ–‡ä»¶
        csv_files = list(self.data_dir.rglob("*.csv"))
        all_files.extend(csv_files)
        
        # æŸ¥æ‰¾Excelæ–‡ä»¶
        excel_files = list(self.data_dir.rglob("*.xlsx"))
        all_files.extend(excel_files)
        
        excel_files.extend(list(self.data_dir.rglob("*.xls")))
        all_files.extend(excel_files)
        
        logger.info(f"æ‰¾åˆ° {len(all_files)} ä¸ªæ–‡ä»¶")
        
        # å¯¼å…¥æ–‡ä»¶
        for file_path in all_files:
            self.stats['total_files'] += 1
            imported = self.import_file(file_path)
            if imported > 0:
                self.stats['imported_files'] += 1
            else:
                self.stats['failed_files'] += 1
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report()
    
    def generate_report(self):
        """ç”Ÿæˆå¯¼å…¥æŠ¥å‘Š"""
        report = {
            'import_time': datetime.now().isoformat(),
            'statistics': self.stats,
            'database_info': DB_CONFIG
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = Path("logs/import_report.json")
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # æ‰“å°æ‘˜è¦
        logger.info("="*80)
        logger.info("æ•°æ®å¯¼å…¥å®Œæˆï¼")
        logger.info(f"æ€»æ–‡ä»¶æ•°: {self.stats['total_files']}")
        logger.info(f"æˆåŠŸå¯¼å…¥: {self.stats['imported_files']}")
        logger.info(f"å¯¼å…¥å¤±è´¥: {self.stats['failed_files']}")
        logger.info(f"æ€»è®°å½•æ•°: {self.stats['imported_records']:,}")
        logger.info("æŒ‰ç±»åˆ«ç»Ÿè®¡:")
        for category, count in self.stats['categories'].items():
            logger.info(f"  {category}: {count:,}")
        logger.info("æŒ‰åœ°åŒºç»Ÿè®¡:")
        for region, count in self.stats['regions'].items():
            logger.info(f"  {region}: {count:,}")
        logger.info(f"è¯¦ç»†æŠ¥å‘Š: {report_file}")
        logger.info("="*80)

class DataQueryService:
    """æ•°æ®æŸ¥è¯¢æœåŠ¡"""
    
    def __init__(self):
        self.conn = None
        
    def get_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥"""
        if not self.conn:
            self.conn = psycopg2.connect(**DB_CONFIG)
        return self.conn
    
    def get_total_count(self) -> Dict[str, Any]:
        """è·å–æ€»è®°å½•æ•°"""
        cursor = self.get_connection().cursor()
        cursor.execute("SELECT COUNT(*) FROM persons")
        total = cursor.fetchone()[0]
        
        # æŒ‰å›½å®¶ç»Ÿè®¡
        cursor.execute("""
            SELECT country, COUNT(*) 
            FROM persons 
            WHERE country IS NOT NULL
            GROUP BY country
        """)
        by_country = dict(cursor.fetchall())
        
        # æŒ‰åŸå¸‚ç»Ÿè®¡
        cursor.execute("""
            SELECT city, COUNT(*) 
            FROM persons 
            WHERE city IS NOT NULL
            GROUP BY city
        """)
        by_city = dict(cursor.fetchall())
        
        return {
            'total': total,
            'by_country': by_country,
            'by_city': by_city
        }
    
    def search_by_name(self, name: str, limit: int = 10) -> List[Dict[str, Any]]:
        """æŒ‰å§“åæœç´¢"""
        cursor = self.get_connection().cursor()
        cursor.execute("""
            cursor.execute("""
            SELECT name, phone, email, city, country, is_chinese, chinese_score
            FROM persons 
            WHERE name ILIKE %s 
        """, (f"%{name}%",))
        
        columns = [desc[0] for desc in cursor.description]
        return [dict(zip(columns, row)) for row in cursor.fetchall()]
    
    def search_by_category(self, category: str, limit: int = 10) -> List[Dict[str, Any]]:
        """æŒ‰å›½å®¶æœç´¢"""
        cursor = self.get_connection().cursor()
        cursor.execute("""
            SELECT name, phone, email, city, country, is_chinese
            FROM persons 
            WHERE country = %s
            ORDER BY chinese_score DESC
            LIMIT %s
        """, (category, limit))
        
        columns = [desc[0] for desc in cursor.description]
        return [dict(zip(columns, row)) for row in cursor.fetchall()]
    
    def search_by_region(self, region: str, limit: int = 10) -> List[Dict[str, Any]]:
        """æŒ‰åŸå¸‚æœç´¢"""
        cursor = self.get_connection().cursor()
        cursor.execute("""
            SELECT name, phone, email, city, country, is_chinese
            FROM persons 
            WHERE city = %s
            ORDER BY chinese_score DESC
            LIMIT %s
        """, (region, limit))
        
        columns = [desc[0] for desc in cursor.description]
        return [dict(zip(columns, row)) for row in cursor.fetchall()]

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='BossJy-Pro æ•°æ®å¯¼å…¥å·¥å…·')
    parser.add_argument('--import-data', action='store_true', help='å¯¼å…¥æ‰€æœ‰æ•°æ®')
    parser.add_argument('--stats', action='store_true', help='æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯')
    parser.add_argument('--search', type=str, help='æœç´¢å§“å')
    parser.add_argument('--category', type=str, help='æŒ‰ç±»åˆ«æœç´¢')
    parser.add_argument('--region', type=str, help='æŒ‰åœ°åŒºæœç´¢')
    
    args = parser.parse_args()
    
    if args.import_data:
        # å¯¼å…¥æ•°æ®
        importer = DataImporter()
        importer.import_all_data()
    
    elif args.stats or args.search or args.category or args.region:
        # æŸ¥è¯¢æ•°æ®
        query_service = DataQueryService()
        
        if args.stats:
            stats = query_service.get_total_count()
            print("æ•°æ®ç»Ÿè®¡:")
            print(f"æ€»è®°å½•æ•°: {stats['total']:,}")
            print("\næŒ‰å›½å®¶:")
            for country, count in stats['by_country'].items():
                print(f"  {country}: {count:,}")
            print("\næŒ‰åŸå¸‚:")
            for city, count in stats['by_city'].items():
                print(f"  {city}: {count:,}")
            
        
        elif args.search:
            results = query_service.search_by_name(args.search)
            print(f"ğŸ” æœç´¢ '{args.search}' çš„ç»“æœ:")
            for result in results:
                print(f"  å§“å: {result['original_name']}")
                print(f"  ä¸­æ–‡: {result['chinese_name']}")
                print(f"  ç”µè¯: {result['phone_number']}")
                print(f"  é‚®ç®±: {result['email']}")
                print(f"  åœ°åŒº: {result['region']}")
                print(f"  ç±»åˆ«: {result['category']}")
                print("-" * 40)
        
        elif args.category:
            results = query_service.search_by_category(args.category)
            print(f"ğŸ“‚ ç±»åˆ« '{args.category}' çš„è®°å½•:")
            for result in results:
                print(f"  å§“å: {result['original_name']}")
                print(f"  ä¸­æ–‡: {result['chinese_name']}")
                print(f"  ç”µè¯: {result['phone_number']}")
                print(f"  åŸå¸‚: {result['city']}")
                print("-" * 40)
        
        elif args.region:
            results = query_service.search_by_region(args.region)
            print(f"ğŸŒ åœ°åŒº '{args.region}' çš„è®°å½•:")
            for result in results:
                print(f"  å§“å: {result['original_name']}")
                print(f"  ä¸­æ–‡: {result['chinese_name']}")
                print(f"  ç”µè¯: {result['phone_number']}")
                print(f"  ç±»åˆ«: {result['category']}")
                print("-" * 40)
    
    else:
        parser.print_help()

if __name__ == "__main__":
    main()
