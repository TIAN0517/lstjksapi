#!/usr/bin/env python3
"""
æ‰¹é‡æ•°æ®å¯¼å…¥è„šæœ¬ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
åŠŸèƒ½ï¼š
1. æ‰«ææ‰€æœ‰æ•°æ®æ–‡ä»¶
2. åˆ é™¤æ—¥æœŸç›¸å…³å­—æ®µï¼ˆä¿æŠ¤æ•°æ®æ—¶æ•ˆæ€§ï¼‰
3. åˆ†ç±»æ•´ç†æ•°æ®
4. æ‰¹é‡å¯¼å…¥æ•°æ®å¸‚åœºæ•°æ®åº“ï¼ˆä½¿ç”¨æ‰¹é‡æ’å…¥ï¼Œå¸¦è¿›åº¦æ˜¾ç¤ºï¼‰
"""

import pandas as pd
import os
import sys
import sqlite3
from pathlib import Path
import logging
import json
from tqdm import tqdm

sys.path.append('/mnt/d/BossJy-Cn/BossJy-Cn')

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/mnt/d/BossJy-Cn/BossJy-Cn/logs/bulk_import_optimized.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# æ•°æ®åº“è·¯å¾„
DB_PATH = '/mnt/d/BossJy-Cn/BossJy-Cn/data/marketplace.db'

# æ—¥æœŸç›¸å…³å­—æ®µçš„å…³é”®è¯ï¼ˆä¸­è‹±æ–‡ï¼‰
DATE_KEYWORDS = [
    'date', 'time', 'year', 'month', 'day',
    'æ—¥æœŸ', 'æ—¶é—´', 'å¹´ä»½', 'æœˆä»½', 'å¹´',
    'created', 'updated', 'modified',
    'åˆ›å»º', 'æ›´æ–°', 'ä¿®æ”¹',
    'register', 'registration',
    'æ³¨å†Œ',
    'birth', 'birthday',
    'ç”Ÿæ—¥', 'å‡ºç”Ÿ'
]

def is_date_column(column_name):
    """åˆ¤æ–­æ˜¯å¦ä¸ºæ—¥æœŸç›¸å…³å­—æ®µ"""
    column_lower = str(column_name).lower()
    for keyword in DATE_KEYWORDS:
        if keyword in column_lower:
            return True
    return False

def remove_date_columns(df):
    """åˆ é™¤æ•°æ®æ¡†ä¸­çš„æ—¥æœŸç›¸å…³å­—æ®µ"""
    date_columns = [col for col in df.columns if is_date_column(col)]

    if date_columns:
        logger.info(f"  ğŸ—‘ï¸  åˆ é™¤æ—¥æœŸå­—æ®µ: {', '.join(date_columns)}")
        df = df.drop(columns=date_columns)

    return df, date_columns

def classify_data_type(file_path):
    """æ ¹æ®æ–‡ä»¶è·¯å¾„å’Œå†…å®¹åˆ¤æ–­æ•°æ®ç±»å‹"""
    file_name = os.path.basename(file_path).lower()
    file_path_lower = file_path.lower()

    # æ ¹æ®æ–‡ä»¶ååˆ¤æ–­
    if 'hongkong' in file_name or 'hong_kong' in file_name or 'é¦™æ¸¯' in file_name:
        return 'hongkong'
    elif 'australia' in file_name or 'æ¾³' in file_name or 'au' in file_name:
        return 'australia'
    elif 'indonesia' in file_name or 'å°å°¼' in file_name:
        return 'indonesia'
    elif 'canada' in file_name or 'åŠ æ‹¿å¤§' in file_name:
        return 'chinese_global'
    elif 'united_states' in file_name or 'usa' in file_name or 'ç¾å›½' in file_name:
        return 'chinese_global'
    elif 'germany' in file_name or 'å¾·å›½' in file_name:
        return 'chinese_global'
    elif 'united_kingdom' in file_name or 'uk' in file_name or 'è‹±å›½' in file_name:
        return 'chinese_global'
    elif 'italy' in file_name or 'æ„å¤§åˆ©' in file_name:
        return 'chinese_global'
    elif 'chinese' in file_name or 'åäºº' in file_name:
        return 'chinese_global'

    # é»˜è®¤å½’ç±»ä¸ºå…¨çƒåäººæ•°æ®
    return 'chinese_global'

def batch_insert_data(conn, data_list, data_type, batch_size=1000):
    """æ‰¹é‡æ’å…¥æ•°æ®åˆ°æ•°æ®åº“"""
    cursor = conn.cursor()

    inserted = 0
    for i in range(0, len(data_list), batch_size):
        batch = data_list[i:i+batch_size]

        rows = []
        for item in batch:
            data = item['data']
            raw_data_json = json.dumps(data, ensure_ascii=False)
            rows.append((
                data_type,
                data.get('name', ''),
                data.get('company', ''),
                data.get('phone', ''),
                data.get('email', ''),
                data.get('address', ''),
                data.get('city', ''),
                data.get('country', ''),
                raw_data_json,
                item['is_sample'],
                item['price']
            ))

        cursor.executemany(
            '''INSERT INTO data_marketplace
               (data_type, name, company, phone, email, address, city, country, raw_data, is_sample, price)
               VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)''',
            rows
        )
        inserted += len(rows)

    conn.commit()
    return inserted

def process_and_import_file(file_path, conn):
    """å¤„ç†å•ä¸ªæ–‡ä»¶å¹¶æ‰¹é‡å¯¼å…¥æ•°æ®åº“"""
    try:
        logger.info(f"\nğŸ“ å¤„ç†æ–‡ä»¶: {os.path.basename(file_path)}")

        # è¯»å–æ–‡ä»¶
        if file_path.endswith(('.xlsx', '.xls')):
            df = pd.read_excel(file_path)
        elif file_path.endswith('.csv'):
            df = pd.read_csv(file_path)
        else:
            logger.warning(f"  âš ï¸  è·³è¿‡ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼")
            return 0, 0

        original_rows = len(df)
        original_cols = len(df.columns)

        if original_rows == 0:
            logger.warning(f"  âš ï¸  æ–‡ä»¶ä¸ºç©ºï¼Œè·³è¿‡")
            return 0, 0

        # åˆ é™¤æ—¥æœŸå­—æ®µ
        df, removed_cols = remove_date_columns(df)

        # åˆ¤æ–­æ•°æ®ç±»å‹
        data_type = classify_data_type(file_path)
        logger.info(f"  ğŸ“Š æ•°æ®ç±»å‹: {data_type}")
        logger.info(f"  ğŸ“ˆ æ•°æ®é‡: {len(df)} æ¡è®°å½•ï¼Œ{len(df.columns)} ä¸ªå­—æ®µ")

        if len(removed_cols) > 0:
            logger.info(f"  âœ‚ï¸  å·²åˆ é™¤ {len(removed_cols)} ä¸ªæ—¥æœŸå­—æ®µ")

        # ç¡®å®šæ ·æœ¬æ•°é‡
        sample_count = min(10, len(df))

        # å‡†å¤‡æ‰¹é‡æ•°æ®
        data_list = []
        logger.info(f"  ğŸ”„ å‡†å¤‡æ•°æ®...")

        for idx, row in tqdm(df.iterrows(), total=len(df), desc="  å¤„ç†è®°å½•"):
            # è½¬æ¢ä¸ºå­—å…¸å¹¶æ¸…ç†ç©ºå€¼
            data = {k: v for k, v in row.to_dict().items() if pd.notna(v)}

            # å‰10æ¡ä½œä¸ºæ ·æœ¬
            is_sample = idx < sample_count

            data_list.append({
                'data': data,
                'is_sample': is_sample,
                'price': 99.0
            })

        # æ‰¹é‡å¯¼å…¥
        logger.info(f"  ğŸ’¾ å¼€å§‹æ‰¹é‡å¯¼å…¥...")
        imported_count = batch_insert_data(conn, data_list, data_type, batch_size=9001)
        sample_imported = sample_count

        logger.info(f"  âœ… æˆåŠŸå¯¼å…¥ {imported_count} æ¡æ•°æ®ï¼ˆå…¶ä¸­ {sample_imported} æ¡æ ·æœ¬ï¼‰")
        return imported_count, sample_imported

    except Exception as e:
        logger.error(f"  âŒ å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return 0, 0

def main():
    """ä¸»å‡½æ•°"""
    logger.info("=" * 60)
    logger.info("ğŸš€ æ‰¹é‡æ•°æ®å¯¼å…¥å¼€å§‹ï¼ˆä¼˜åŒ–ç‰ˆï¼‰")
    logger.info("=" * 60)

    # è¿æ¥æ•°æ®åº“
    conn = sqlite3.connect(DB_PATH)

    # æ‰«ææ•°æ®ç›®å½•
    data_dirs = [
        '/mnt/d/BossJy-Cn/BossJy-Cn/data/exports',
        '/mnt/d/BossJy-Cn/BossJy-Cn/data/embeddings'
    ]

    all_files = []
    for data_dir in data_dirs:
        if os.path.exists(data_dir):
            for root, dirs, files in os.walk(data_dir):
                for file in files:
                    if file.endswith(('.xlsx', '.xls', '.csv')):
                        # è·³è¿‡æŠ¥å‘Šæ–‡ä»¶
                        if 'report' not in file.lower() and 'embedding' not in file.lower():
                            all_files.append(os.path.join(root, file))

    logger.info(f"\nğŸ“‹ æ‰¾åˆ° {len(all_files)} ä¸ªæ•°æ®æ–‡ä»¶")

    # ç»Ÿè®¡
    total_imported = 0
    total_samples = 0
    processed_files = 0

    # å¤„ç†æ¯ä¸ªæ–‡ä»¶
    for file_path in all_files:
        imported, samples = process_and_import_file(file_path, conn)
        total_imported += imported
        total_samples += samples
        if imported > 0:
            processed_files += 1

    # å…³é—­æ•°æ®åº“è¿æ¥
    conn.close()

    # æ˜¾ç¤ºç»Ÿè®¡
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“Š å¯¼å…¥ç»Ÿè®¡")
    logger.info("=" * 60)
    logger.info(f"å¤„ç†æ–‡ä»¶æ•°: {processed_files} / {len(all_files)}")
    logger.info(f"å¯¼å…¥æ€»æ•°æ®: {total_imported} æ¡")
    logger.info(f"å¯¼å…¥æ ·æœ¬æ•°: {total_samples} æ¡")

    # æŸ¥è¯¢æ•°æ®å¸‚åœºç»Ÿè®¡
    logger.info("\nğŸ“ˆ æ•°æ®å¸‚åœºç»Ÿè®¡:")
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()

    type_names = {
        'hongkong': 'ğŸ‡­ğŸ‡° æ¾³æ´²é¦™æ¸¯äººæ•°æ®',
        'australia': 'ğŸ‡¦ğŸ‡º æ¾³æ´²æœ¬åœ°å®¢æˆ·æ•°æ®',
        'indonesia': 'ğŸ‡®ğŸ‡© å°å°¼åäººæ•°æ®',
        'chinese_global': 'ğŸ‡¨ğŸ‡³ å…¨çƒåäººæ•°æ®'
    }

    for data_type, type_name in type_names.items():
        cursor.execute(
            'SELECT COUNT(*) FROM data_marketplace WHERE data_type = ?',
            (data_type,)
        )
        total = cursor.fetchone()[0]

        cursor.execute(
            'SELECT COUNT(*) FROM data_marketplace WHERE data_type = ? AND is_sample = 1',
            (data_type,)
        )
        samples = cursor.fetchone()[0]

        if total > 0:
            logger.info(f"  {type_name}")
            logger.info(f"    â€¢ æ€»æ•°æ®: {total} æ¡")
            logger.info(f"    â€¢ æ ·æœ¬æ•°: {samples} æ¡")

    conn.close()

    logger.info("\nâœ… æ‰¹é‡å¯¼å…¥å®Œæˆï¼")

if __name__ == '__main__':
    main()
