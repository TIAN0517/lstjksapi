# -*- coding: utf-8 -*-
"""
hk_evolution_full.py
æœ€çµ‚å„ªåŒ–ç‰ˆï¼šå¤§è¦æ¨¡æ•´ä½µå¤šæ ¼å¼åå–®ï¼Œä¾†æºå¯è¿½æº¯ï¼ˆæª”å/å·¥ä½œè¡¨/åœ‹å®¶ï¼‰ï¼Œ
å®‰å…¨å­—ä¸²åŒ–ã€è¨˜æ†¶é«”å‹å–„ï¼Œä¸¦è¼¸å‡ºæ¸…æ™°å½™ç¸½èˆ‡åˆ†é¡ã€‚

ä¾è³´ï¼š
  pip install -U pandas openpyxl xlrd
  pip install pyexcel-xls pyexcel-io
  ï¼ˆå¯é¸ï¼‰pip install colorama tqdm

åŸ·è¡Œï¼š
  python hk_evolution_full.py [è³‡æ–™å¤¾è·¯å¾‘]

è¼¸å‡ºï¼š
  ./output/integrated_master.csv
  ./output/hk_in_au.csv / ./output/uncertain.csv / ./output/not_hk.csv
  ./output/email_domain_stats.csv / ./output/surname_stats.csv
  ./output/label_counts.csv / ./output/country_label_pivot.csv
  å‹•æ…‹ç‰¹å¾µåº«ï¼šdynamic_*.txt æœƒåœ¨å·¥ä½œç›®éŒ„ç¶­è­·/å¢è£œ

å‚™è¨»ï¼š
- ä»¥ä¸²æµå¯«æª”é¿å…ä¸€æ¬¡æ€§ä½µå·¨å¤§ DataFrame çš„è¨˜æ†¶é«”ç‚¸è£‚
- .xls å„ªå…ˆå˜—è©¦ pandas+xlrdï¼Œä¸è¡Œè‡ªå‹•æ”¹ç”¨ pyexcel-xls
- æ¬„åå…ˆè½‰å­—ä¸²â†’æ­£è¦åŒ–å°é½Šåˆ° name/email/phone/addressï¼Œå†åšè©•åˆ†
"""

import os, re, sys, glob, math, warnings
from pathlib import Path
from collections import Counter, defaultdict

import pandas as pd

# å¯é¸å¥—ä»¶
try:
    from tqdm import tqdm
except Exception:
    tqdm = None

try:
    from colorama import init as colorama_init, Fore, Style
    colorama_init(autoreset=True)
except Exception:
    class _Dummy: 
        RESET_ALL = ""
    class _C:
        RED = GREEN = YELLOW = CYAN = MAGENTA = BLUE = WHITE = ""
    Fore = _C()
    Style = _Dummy()

warnings.filterwarnings("ignore", category=UserWarning)

# ---------------------------
# å¯èª¿åƒæ•¸
# ---------------------------
CHUNK_SIZE_CSV = 100_000       # CSV åˆ†å¡Šå¤§å°
MAX_REASONABLE_COLUMNS = 2000   # æ‹’æ”¶éå¯¬è¡¨æ ¼ï¼ˆç–‘ä¼¼è§£æéŒ¯èª¤ï¼‰
OUT_DIR = Path("./output")
OUT_DIR.mkdir(parents=True, exist_ok=True)

MASTER_CSV = OUT_DIR / "integrated_master.csv"
HK_IN_AU_CSV = OUT_DIR / "hk_in_au.csv"
UNCERTAIN_CSV = OUT_DIR / "uncertain.csv"
NOT_HK_CSV = OUT_DIR / "not_hk.csv"

SURNAME_STATS_CSV = OUT_DIR / "surname_stats.csv"
DOMAIN_STATS_CSV = OUT_DIR / "email_domain_stats.csv"
LABEL_COUNTS_CSV = OUT_DIR / "label_counts.csv"
COUNTRY_LABEL_PIVOT_CSV = OUT_DIR / "country_label_pivot.csv"

# å¿…è¦æ¬„ä½æ¨™æº–å
NEEDED_COLS = ["name", "email", "phone", "address"]

# æ¬„ä½æ˜ å°„ï¼ˆå¸¸è¦‹è®Šé«” -> æ¨™æº–åï¼‰
COL_MAPS = {
    # name
    "name": "name", "å§“å": "name", "full name": "name", "first name": "name", "last name": "name",
    "contact name": "name", "person": "name", "user": "name", "å®¢æˆ·": "name", "åç¨±": "name",
    # email
    "email": "email", "e-mail": "email", "mail": "email", "éƒµç®±": "email", "é‚®ç®±": "email",
    "é›»å­éƒµä»¶": "email", "é›»å­éƒµç®±": "email",
    # phone
    "phone": "phone", "tel": "phone", "telephone": "phone", "mobile": "phone", "cell": "phone",
    "cellphone": "phone", "æ‰‹æ©Ÿ": "phone", "æ‰‹æœºå·": "phone", "é›»è©±": "phone",
    # address
    "address": "address", "addr": "address", "ä½å€": "address", "åœ°å€": "address", "è¡—é“": "address",
    "location": "address"
}

# ---------------------------
# åŸºç¤ç‰¹å¾µåº«ï¼ˆå¯è¢«å‹•æ…‹ list ç–ŠåŠ ï¼‰
# ---------------------------
base_hk_surnames = [
    "chan","cheung","wong","lee","ho","ng","yuen","chiu","lam","kwok","leung","mak",
    "chu","tsang","chow","au","fung","kwan","ip","lo","man","tam","to","wan","yu","yip",
    "lau","poon","pang","hui","cheng","shum","shek","ngai","lok","ko","law"
]
base_cantonese_spellings = [
    "cheung","chow","tsang","chu","yuen","kwok","wai","ho","man","poon","ng"
]
base_hk_domains = [
    "hku.hk","cuhk.edu.hk","hkbu.edu.hk","polyu.edu.hk",
    "cityu.edu.hk","ln.edu.hk","eduhk.hk","ust.hk",
    "netvigator.com","yahoo.com.hk","hotmail.com.hk"
]
base_hk_suburbs = [
    "chatswood","hurstville","eastwood","burwood",
    "box hill","glen waverley","doncaster","rhodes"
]
base_hk_phones = ["+852","852"]

# ---------------------------
# å·¥å…·ï¼šé¡è‰² log
# ---------------------------
def info(msg): print(Fore.CYAN + msg + Style.RESET_ALL)
def good(msg): print(Fore.GREEN + "âœ… " + msg + Style.RESET_ALL)
def warn(msg): print(Fore.YELLOW + "âš ï¸ " + msg + Style.RESET_ALL)
def bad(msg):  print(Fore.RED + "âŒ " + msg + Style.RESET_ALL)

# ---------------------------
# æª”åæ¨æ¸¬åœ‹åˆ¥ï¼ˆå¯è‡ªè¡Œæ“´å……ï¼‰
# ---------------------------
COUNTRY_MAP = {
    # è‹±æ–‡ç¸®å¯«
    "AU": "AU", "AUS": "AU", "Australia": "AU",
    "UK": "UK", "GB": "UK", "GBR": "UK",
    "CA": "CA", "CAN": "CA", "Canada": "CA",
    "DE": "DE", "GER": "DE", "Germany": "DE",
    "IT": "IT", "ITA": "IT", "Italy": "IT",
    "FR": "FR", "FRA": "FR", "France": "FR",
    "US": "US", "USA": "US",
    "PL": "PL", "POL": "PL",
    # å¸¸è¦‹ä¸­æ–‡ï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰
    "æ¾³": "AU", "è‹±": "UK", "å¾·": "DE", "ç¾©å¤§åˆ©": "IT", "æ„å¤§åˆ©": "IT", "æ³•": "FR", "åŠ ": "CA", "ç¾": "US",
}

def guess_country_code_from_filename(path: str) -> str:
    name = Path(path).name
    stem = Path(path).stem
    tokens = re.split(r"[^\w\u4e00-\u9fff]+", stem)
    candidates = []
    for t in tokens:
        if not t: 
            continue
        t_up = t.upper()
        t_cn = t
        for k, v in COUNTRY_MAP.items():
            if len(k) <= 3:  # è‹±æ–‡çŸ­ç¢¼
                if t_up == k or k in t_up:
                    candidates.append(v)
            else:  # ä¸­æ–‡è©
                if k in t_cn:
                    candidates.append(v)
    if not candidates:
        # ç¶²åŸŸç·šç´¢
        if ".com.au" in name.lower() or name.lower().endswith("_au"):
            return "AU"
    return candidates[0] if candidates else ""

# ---------------------------
# å‹•æ…‹ç‰¹å¾µåº«
# ---------------------------
def load_dynamic_list(file, base_list):
    if os.path.exists(file):
        with open(file, "r", encoding="utf-8", errors="ignore") as f:
            dynamic = [x.strip().lower() for x in f if x.strip()]
        return list(dict.fromkeys(base_list + dynamic))  # å»é‡ä¿åº
    return base_list

def save_new_candidates(counter: Counter, file: str, threshold: int, existing: set):
    new_items = [it for it, c in counter.items() if c >= threshold and it and it not in existing]
    if new_items:
        with open(file, "a", encoding="utf-8") as f:
            for it in new_items:
                f.write(it + "\n")
        good(f"âœ¨ æ–°å¢ {len(new_items)} é …åˆ° {file}")

# ---------------------------
# æ¬„ä½æ­£è¦åŒ–
# ---------------------------
def to_str_series(s: pd.Series) -> pd.Series:
    # å®‰å…¨å­—ä¸²åŒ–ï¼šNaN â†’ "", å…¶ä»– â†’ str
    return s.map(lambda x: "" if pd.isna(x) else (str(x).strip()))

def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    # æ¬„åä¸€å¾‹è½‰å­—ä¸²ï¼Œé¿å… 'int' æ²’æœ‰ strip()
    df.columns = [str(c) for c in df.columns]

    if df.shape[1] > MAX_REASONABLE_COLUMNS:
        df.attrs["_too_wide"] = True
        return df

    rename = {}
    for c in df.columns:
        c_norm = str(c).strip().lower()
        rename[c] = COL_MAPS.get(c_norm, c_norm)
    df = df.rename(columns=rename)

    # ç¼ºçš„è£œé½Š
    for col in NEEDED_COLS:
        if col not in df.columns:
            df[col] = ""

    # åªä¿ç•™å¿…è¦æ¬„ä½ + å…¶é¤˜ï¼ˆç¶­æŒåŸé †åºï¼‰
    ordered = NEEDED_COLS + [c for c in df.columns if c not in NEEDED_COLS]
    df = df[ordered]

    # å¿…è¦æ¬„ä½å¼·åˆ¶å­—ä¸²åŒ–
    for col in NEEDED_COLS:
        df[col] = to_str_series(df[col])

    return df

# ---------------------------
# å„æ ¼å¼è®€å–ï¼ˆä¸²æµ/yield DataFrameï¼‰
# ---------------------------
def iter_from_csv(file: str):
    total_rows = None
    # å˜—è©¦ä¼°è¡Œæ•¸ï¼ˆä¸ä¿è­‰æº–ç¢ºï¼Œåªç‚ºé€²åº¦æ¢ï¼‰
    try:
        with open(file, "rb") as f:
            total_rows = sum(1 for _ in f)
    except Exception:
        pass

    encodings_try = ["utf-8", "utf-8-sig", "latin-1"]
    for enc in encodings_try:
        try:
            it = pd.read_csv(
                file,
                dtype=str,
                on_bad_lines="skip",
                low_memory=False,
                chunksize=CHUNK_SIZE_CSV,
                encoding=enc
            )
            iterator = it
            break
        except Exception as e:
            warn(f"{file} ä»¥ {enc} è®€å–å¤±æ•—ï¼š{e}")
            iterator = None
    if iterator is None:
        bad(f"{file} CSV è®€å–å¤±æ•—ï¼Œè·³é")
        return

    iterable = iterator
    if tqdm:
        iterable = tqdm(iterator, desc=f"ğŸ“¥ {Path(file).name}", unit="chunk")

    for chunk in iterable:
        if chunk is None or chunk.empty:
            continue
        chunk = chunk.fillna("")
        chunk = normalize_columns(chunk)
        if chunk.attrs.get("_too_wide"):
            bad(f"{file} æ¬„ä½æ•¸ç•°å¸¸ï¼ˆ>{MAX_REASONABLE_COLUMNS}ï¼‰ï¼Œè·³é")
            continue
        yield chunk

def iter_from_xlsx(file: str):
    try:
        xls = pd.ExcelFile(file, engine="openpyxl")
    except Exception as e:
        bad(f"{file} è®€ .xlsx å¤±æ•—ï¼š{e}")
        return

    sheets = xls.sheet_names
    iterable = sheets
    if tqdm:
        iterable = tqdm(sheets, desc=f"ğŸ“— {Path(file).name}", unit="sheet")

    for sh in iterable:
        try:
            df = pd.read_excel(file, sheet_name=sh, dtype=str, engine="openpyxl")
            if df is None or df.empty:
                continue
            df = df.fillna("")
            df = normalize_columns(df)
            if df.attrs.get("_too_wide"):
                bad(f"{file}::{sh} æ¬„ä½æ•¸ç•°å¸¸ï¼ˆ>{MAX_REASONABLE_COLUMNS}ï¼‰ï¼Œè·³é")
                continue
            df["sheet_name"] = sh
            yield df
        except Exception as e:
            bad(f"{file}::'{sh}' è®€å–å¤±æ•—ï¼š{e}")

def read_xls_via_pyexcel(file: str):
    try:
        from pyexcel_xls import get_data
    except Exception as e:
        bad(f"{file} éœ€è¦å¥—ä»¶ pyexcel-xlsï¼šè«‹å…ˆ `pip install pyexcel-xls pyexcel-io`ï¼›éŒ¯èª¤ï¼š{e}")
        return []

    try:
        data = get_data(file)  # dict: sheet_name -> rows (list[list])
    except Exception as e:
        bad(f"{file} ä½¿ç”¨ pyexcel-xls è§£æå¤±æ•—ï¼š{e}")
        return []

    frames = []
    for sh, rows in data.items():
        if not rows:
            continue
        header = rows[0]
        # è‹¥é¦–åˆ—éå­—ä¸²æ¬„åï¼Œç”Ÿæˆé€šç”¨æ¬„å
        if any(isinstance(x, str) and x.strip() for x in header):
            cols = [str(x).strip() if x is not None else f"col{i}" for i, x in enumerate(header)]
            body = rows[1:]
        else:
            cols = [f"col{i}" for i in range(len(header))]
            body = rows
        df = pd.DataFrame(body, columns=cols)
        df = df.fillna("")
        df = normalize_columns(df)
        if df.attrs.get("_too_wide"):
            bad(f"{file}::{sh} æ¬„ä½æ•¸ç•°å¸¸ï¼ˆ>{MAX_REASONABLE_COLUMNS}ï¼‰ï¼Œè·³é")
            continue
        df["sheet_name"] = sh
        frames.append(df)
    return frames

def iter_from_xls(file: str):
    # å„ªå…ˆè©¦ xlrd
    try:
        df = pd.read_excel(file, dtype=str, engine="xlrd")
        df = df.fillna("")
        df = normalize_columns(df)
        if not df.empty:
            df["sheet_name"] = ""
            yield df
            return
    except Exception as e1:
        warn(f"{file} xlrd è®€ .xls å¤±æ•—ï¼š{e1}ï¼›æ”¹ç”¨ pyexcel-xls")

    # fallback pyexcel-xls
    frames = read_xls_via_pyexcel(file)
    for df in frames:
        yield df

def iter_frames_from_file(file: str):
    file_l = file.lower()
    if file_l.endswith(".csv"):
        yield from iter_from_csv(file)
    elif file_l.endswith(".xlsx"):
        yield from iter_from_xlsx(file)
    elif file_l.endswith(".xls"):
        yield from iter_from_xls(file)
    elif file_l.endswith(".txt"):
        # å˜—è©¦ tab/é€—è™Ÿ
        for sep in ["\t", ",", ";", "|"]:
            try:
                it = pd.read_csv(file, sep=sep, dtype=str, on_bad_lines="skip", encoding="utf-8", low_memory=False)
                if it is None or it.empty:
                    continue
                it = it.fillna("")
                it = normalize_columns(it)
                if it.attrs.get("_too_wide"):
                    bad(f"{file} æ¬„ä½æ•¸ç•°å¸¸ï¼ˆ>{MAX_REASONABLE_COLUMNS}ï¼‰ï¼Œè·³é")
                    continue
                yield it
                break
            except Exception:
                continue
    else:
        warn(f"{file} æœªæ”¯æ´æ ¼å¼ï¼Œè·³é")

# ---------------------------
# è©•åˆ†ï¼ˆå‘é‡åŒ– + å®‰å…¨å­—ä¸²ï¼‰
# ---------------------------
def build_patterns(hk_surnames, cantonese_spellings, hk_domains, hk_suburbs):
    if hk_surnames:
        pat_surname = r"\b(?:%s)\b" % "|".join(re.escape(x) for x in hk_surnames)
    else:
        pat_surname = ""
    if cantonese_spellings:
        pat_canton = r"(?:%s)" % "|".join(re.escape(x) for x in cantonese_spellings)
    else:
        pat_canton = ""
    if hk_domains:
        pat_domains = r"(?:%s)" % "|".join(re.escape(x) for x in hk_domains)
    else:
        pat_domains = ""
    if hk_suburbs:
        pat_suburbs = r"(?:%s)" % "|".join(re.escape(x) for x in hk_suburbs)
    else:
        pat_suburbs = ""
    return pat_surname, pat_canton, pat_domains, pat_suburbs

def score_block(df: pd.DataFrame,
                hk_surnames, cantonese_spellings, hk_domains, hk_suburbs, hk_phones):
    name_lc   = df["name"].astype(str).str.lower()
    email_lc  = df["email"].astype(str).str.lower()
    phone_txt = df["phone"].astype(str).str.strip()
    addr_lc   = df["address"].astype(str).str.lower()

    pat_surname, pat_canton, pat_domains, pat_suburbs = build_patterns(
        hk_surnames, cantonese_spellings, hk_domains, hk_suburbs
    )

    # å„ç‰¹å¾µå¸ƒæ—
    b_hk_surname   = name_lc.str.contains(pat_surname, regex=True, na=False) if pat_surname else pd.Series([False]*len(df))
    b_canton_name  = name_lc.str.contains(pat_canton, regex=True, na=False)  if pat_canton else pd.Series([False]*len(df))
    b_cjk_name     = df["name"].astype(str).str.contains(r"[\u4e00-\u9fff]", regex=True, na=False)

    b_email_domain = (
        email_lc.str.endswith(".hk", na=False) |
        (email_lc.str.contains(pat_domains, regex=True, na=False) if pat_domains else False)
    )
    b_canton_email = email_lc.str.contains(pat_canton, regex=True, na=False) if pat_canton else pd.Series([False]*len(df))

    b_hk_phone_pref = pd.Series([False]*len(df))
    if hk_phones:
        b_hk_phone_pref = phone_txt.apply(lambda x: any(x.startswith(ph) for ph in hk_phones))

    # é¦™æ¸¯æ‰‹æ©Ÿæ®µï¼ˆåŸºæ–¼ä½ åŸå…ˆè¦å‰‡ï¼š4é–‹é ­+8ä½ï¼‰
    b_hk_mobile = phone_txt.str.match(r"^4\d{7}$", na=False)

    b_addr_suburb = addr_lc.str.contains(pat_suburbs, regex=True, na=False) if pat_suburbs else pd.Series([False]*len(df))

    # å„ç‰¹å¾µæ¬Šé‡
    score = (
        b_hk_surname.astype(int)   * 40 +
        b_canton_name.astype(int)  * 20 +
        b_cjk_name.astype(int)     * 20 +
        b_email_domain.astype(int) * 40 +
        b_canton_email.astype(int) * 15 +
        b_hk_phone_pref.astype(int)* 50 +
        b_hk_mobile.astype(int)    * 20 +
        b_addr_suburb.astype(int)  * 30
    )

    # æ¨™ç±¤
    def classify(s):
        if s >= 80:
            return "HK_in_AU"
        elif s >= 45:
            return "Uncertain"
        else:
            return "Not_HK"
    label = score.map(classify)

    # ç†ç”±ï¼ˆé¿å… ufunc å­—ä¸²ç›¸åŠ éŒ¯èª¤ï¼Œç”¨é€åˆ— joinï¼‰
    reasons_cols = {
        "é¦™æ¸¯å¸¸è¦‹å§“æ°": b_hk_surname,
        "ç²µèªæ‹¼éŸ³ï¼ˆå§“åï¼‰": b_canton_name,
        "ä¸­æ–‡å§“å": b_cjk_name,
        "é¦™æ¸¯ä¿¡ç®±/æ•™è‚²åŸŸ": b_email_domain,
        "Email ç²µèªæ‹¼éŸ³": b_canton_email,
        "é¦™æ¸¯é›»è©±å‰ç¶´": b_hk_phone_pref,
        "é¦™æ¸¯æ‰‹æ©Ÿè™Ÿæ®µ": b_hk_mobile,
        "æ¾³æ´²æ¸¯äººå¸¸ä½å€": b_addr_suburb,
    }
    # å…ˆå»ºä¸€å€‹ DataFrame åªæ”¾å­—ä¸²ç‰‡æ®µæˆ–ç©ºå­—ä¸²
    _reasons_df = pd.DataFrame({k: pd.Series([""]*len(df), dtype=object) for k in reasons_cols})
    for k, mask in reasons_cols.items():
        _reasons_df.loc[mask, k] = k

    reasons = _reasons_df.apply(lambda row: "; ".join([x for x in row if x]), axis=1)

    return score, label, reasons

# ---------------------------
# ä¸²æµå¯«æª”ï¼ˆé¿å…ä¸€æ¬¡æ€§ä½µè¡¨ï¼‰
# ---------------------------
def append_csv(path: Path, df: pd.DataFrame):
    header = not path.exists()
    df.to_csv(path, index=False, encoding="utf-8-sig", mode="a", header=header)

# ---------------------------
# ä¸»æµç¨‹
# ---------------------------
def process_all(base_dir: str):
    # å‹•æ…‹ç‰¹å¾µåº«è¼‰å…¥
    hk_surnames = load_dynamic_list("dynamic_surnames.txt", base_hk_surnames)
    cantonese_spellings = load_dynamic_list("dynamic_spellings.txt", base_cantonese_spellings)
    hk_domains = load_dynamic_list("dynamic_domains.txt", base_hk_domains)
    hk_suburbs = load_dynamic_list("dynamic_suburbs.txt", base_hk_suburbs)
    hk_phones = load_dynamic_list("dynamic_phones.txt", base_hk_phones)

    info("ğŸ”§ ç‰¹å¾µåº«ï¼š")
    print(f"  å§“æ° {len(hk_surnames)} / ç²µèªæ‹¼éŸ³ {len(cantonese_spellings)} / åŸŸå {len(hk_domains)} / æ¾³æ´²åœ°å€ {len(hk_suburbs)} / é›»è©±å‰ç¶´ {len(hk_phones)}")

    # ç›®æ¨™æª”æ¡ˆ
    exts = ("*.csv", "*.xlsx", "*.xls", "*.txt")
    files = []
    for ext in exts:
        files.extend(glob.glob(str(Path(base_dir) / "**" / ext), recursive=True))
    if not files:
        warn("æ²’æœ‰æ‰¾åˆ°ä»»ä½•è³‡æ–™æª”")
        return

    info(f"ğŸ“‚ ç™¼ç¾ {len(files)} å€‹æª”æ¡ˆ")

    # çµ±è¨ˆå™¨ï¼ˆä¸²æµç´¯è¨ˆï¼‰
    label_counts = Counter()
    country_label = Counter()
    surname_counter = Counter()
    domain_counter = Counter()

    total_rows_in = 0

    # ç§»é™¤èˆŠè¼¸å‡º
    for p in [MASTER_CSV, HK_IN_AU_CSV, UNCERTAIN_CSV, NOT_HK_CSV,
              SURNAME_STATS_CSV, DOMAIN_STATS_CSV, LABEL_COUNTS_CSV, COUNTRY_LABEL_PIVOT_CSV]:
        try:
            if p.exists():
                p.unlink()
        except Exception:
            pass

    # ä¸»è¿­ä»£
    iterable = files
    if tqdm:
        iterable = tqdm(files, desc="ğŸ§¾ Files", unit="file")

    for file in iterable:
        country_code = guess_country_code_from_filename(file)

        try:
            frame_iter = iter_frames_from_file(file)
            any_yield = False
            for df in frame_iter:
                any_yield = True
                rows_before = len(df)

                # å¢è£œè¿½æº¯æ¬„ä½
                if "sheet_name" not in df.columns:
                    df["sheet_name"] = ""
                df["source_file"] = str(Path(file).as_posix())
                df["country_code"] = country_code

                # è©•åˆ† + æ¨™ç±¤ + ç†ç”±
                score, label, reasons = score_block(
                    df, hk_surnames, cantonese_spellings, hk_domains, hk_suburbs, hk_phones
                )
                df["score"] = score
                df["label"] = label
                df["reasons"] = reasons

                # ç´¯è¨ˆçµ±è¨ˆï¼ˆé¿å…å¤§è¨˜æ†¶é«”ï¼‰
                total_rows_in += rows_before
                label_counts.update(label.tolist())
                if country_code:
                    for lb in label.tolist():
                        country_label[(country_code, lb)] += 1

                # å§“æ°èˆ‡ç¶²åŸŸçµ±è¨ˆï¼ˆå­—ä¸²å®‰å…¨ï¼‰
                first_surnames = df["name"].map(
                    lambda x: (x.strip().split()[0].lower() if isinstance(x, str) and x.strip() else "")
                )
                domains = df["email"].map(
                    lambda x: (x.split("@")[-1].strip().lower() if isinstance(x, str) and "@" in x else "")
                )
                surname_counter.update([s for s in first_surnames if s])
                domain_counter.update([d for d in domains if d])

                # ä¸²æµè¼¸å‡ºï¼šç¸½è¡¨ & ä¸‰åˆ†é¡
                append_csv(MASTER_CSV, df[[
                    "name","email","phone","address",
                    "score","label","reasons","country_code","source_file","sheet_name"
                ]])

                append_csv(HK_IN_AU_CSV, df[df["label"]=="HK_in_AU"][[
                    "name","email","phone","address",
                    "score","reasons","country_code","source_file","sheet_name"
                ]])

                append_csv(UNCERTAIN_CSV, df[df["label"]=="Uncertain"][[
                    "name","email","phone","address",
                    "score","reasons","country_code","source_file","sheet_name"
                ]])

                append_csv(NOT_HK_CSV, df[df["label"]=="Not_HK"][[
                    "name","email","phone","address",
                    "score","reasons","country_code","source_file","sheet_name"
                ]])

            if not any_yield:
                warn(f"{file} æ²’æœ‰å¯ç”¨è³‡æ–™ï¼ˆæˆ–æ ¼å¼ä¸æ”¯æ´ï¼‰")
        except Exception as e:
            bad(f"{file} è®€å–/è™•ç†å¤±æ•—ï¼š{e}")

    # ---------------------------
    # å½™ç¸½è¼¸å‡º
    # ---------------------------
    info("\nğŸ§® è¼¸å‡ºå½™ç¸½...")

    # æ¨™ç±¤çµ±è¨ˆ
    pd.DataFrame(
        [{"label": k, "count": v} for k, v in sorted(label_counts.items(), key=lambda x: (-x[1], x[0]))]
    ).to_csv(LABEL_COUNTS_CSV, index=False, encoding="utf-8-sig")

    # åœ‹å®¶ Ã— æ¨™ç±¤
    # è½‰æˆé€è¦–è¡¨
    country_label_map = defaultdict(dict)
    for (cty, lb), cnt in country_label.items():
        country_label_map[cty][lb] = cnt
    rows = []
    all_labels = sorted({lb for (_, lb) in country_label.keys()})
    for cty, d in sorted(country_label_map.items()):
        row = {"country_code": cty}
        for lb in all_labels:
            row[lb] = d.get(lb, 0)
        row["total"] = sum(d.values())
        rows.append(row)
    if rows:
        pd.DataFrame(rows).to_csv(COUNTRY_LABEL_PIVOT_CSV, index=False, encoding="utf-8-sig")

    # å§“æ° / ç¶²åŸŸ çµ±è¨ˆ
    pd.Series(surname_counter).sort_values(ascending=False).to_csv(SURNAME_STATS_CSV, header=["count"], encoding="utf-8-sig")
    pd.Series(domain_counter).sort_values(ascending=False).to_csv(DOMAIN_STATS_CSV, header=["count"], encoding="utf-8-sig")

    good("\n===== ğŸ“Š åˆ†æå ±å‘Š =====")
    print(f"ğŸ“Œ ç¸½ç­†æ•¸ï¼ˆä¼°ï¼‰ï¼š{total_rows_in}")
    print(f"ğŸŸ¢ é«˜å¯ä¿¡é¦™æ¸¯èƒŒæ™¯ï¼š{label_counts.get('HK_in_AU', 0)}")
    print(f"ğŸŸ¡ ä¸ç¢ºå®šï¼š{label_counts.get('Uncertain', 0)}")
    print(f"ğŸ”´ éé¦™æ¸¯èƒŒæ™¯ï¼š{label_counts.get('Not_HK', 0)}")
    print("\nğŸ“‘ å½™ç¸½è¼¸å‡ºï¼š")
    print(f" - {MASTER_CSV.name}")
    print(f" - {LABEL_COUNTS_CSV.name} / {COUNTRY_LABEL_PIVOT_CSV.name}")
    print(f" - {SURNAME_STATS_CSV.name} / {DOMAIN_STATS_CSV.name}")
    print("ğŸ“‚ åˆ†é¡è¼¸å‡ºï¼šhk_in_au.csv / uncertain.csv / not_hk.csv")
    print("ğŸ§© ç‰¹å¾µåº«ï¼šdynamic_surnames.txt / dynamic_domains.txt / dynamic_suburbs.txt / dynamic_phones.txt")

    # ---------------------------
    # å‹•æ…‹ç‰¹å¾µåº«å¢è£œ
    # ---------------------------
    info("\nğŸ§  æ›´æ–°å‹•æ…‹ç‰¹å¾µåº«å€™é¸...")
    existing_surnames = set(hk_surnames)
    existing_domains = set(hk_domains)
    save_new_candidates(surname_counter, "dynamic_surnames.txt", threshold=50, existing=existing_surnames)
    save_new_candidates(domain_counter, "dynamic_domains.txt", threshold=50, existing=existing_domains)

# ---------------------------
# å…¥å£
# ---------------------------
if __name__ == "__main__":
    base = sys.argv[1] if len(sys.argv) > 1 else "."
    base = str(Path(base).resolve())
    info(f"ğŸš€ é–‹å§‹è™•ç†ï¼š{base}")
    process_all(base)
